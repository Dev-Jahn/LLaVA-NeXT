cuda_devices: "4,5,6,7"
wandb_project: "video"

base_command:
  - accelerate
  - launch
  - --config_file
  - scripts/accelerate.yaml
  - llava/train/train_voco.py

training_params:
  run_name: voco-7b-finetune-4token
  num_voco: 4
  attn_implementation: sdpa
  model_name_or_path: lmsys/vicuna-7b-v1.5
  version: v1
  data_path: ./playground/data/llava_v1_5_mix665k.json
  image_folder: ./playground/data
  vision_tower: openai/clip-vit-large-patch14-336
  pretrain_mm_mlp_adapter: ./checkpoints/llava-v1.5-mlp2x-336px-pretrain-vicuna-7b-v1.5/mm_projector.bin
  mm_projector_type: mlp2x_gelu
  mm_vision_select_layer: -2
  mm_use_im_start_end: false
  mm_use_im_patch_token: false
  image_aspect_ratio: pad
  group_by_modality_length: true
  bf16: true
  output_dir: ./checkpoints/voco-7b-finetune-4token
  num_train_epochs: 2
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 2
  eval_strategy: "no"
  save_strategy: "steps"
  save_steps: 50000
  save_total_limit: 1
  learning_rate: 2e-5
  weight_decay: 0.0
  warmup_ratio: 0.03
  lr_scheduler_type: "cosine"
  logging_steps: 1
  tf32: true
  model_max_length: 2048
  gradient_checkpointing: true
  dataloader_num_workers: 4
  lazy_preprocess: true